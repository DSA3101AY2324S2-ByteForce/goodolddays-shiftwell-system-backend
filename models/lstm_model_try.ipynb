{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Forecast: LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/_l78dvs92m346fbfkb6pl1h40000gn/T/ipykernel_40961/3098886399.py:31: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from itertools import product\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import itertools\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from kerastuner.tuners import Hyperband\n",
    "from tensorflow import keras\n",
    "from scipy.stats import skewnorm\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapping weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weather_data():\n",
    "    try:\n",
    "        # Open the JSON file\n",
    "        with open('weather_data.json', 'r') as file:\n",
    "            # Load data from the JSON file\n",
    "            weather_data = json.load(file)\n",
    "        \n",
    "        # Print the weather data\n",
    "        for entry in weather_data:\n",
    "            print(entry)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the filename and try again.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON. Please check the JSON file for errors.\")\n",
    "\n",
    "# Run the function\n",
    "#read_weather_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weather data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weather Condition</th>\n",
       "      <th>Is Heavy Rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>16:00</td>\n",
       "      <td>30</td>\n",
       "      <td>Thundershowers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>17:00</td>\n",
       "      <td>30</td>\n",
       "      <td>Thundershowers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>18:00</td>\n",
       "      <td>29</td>\n",
       "      <td>Thundershowers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>19:00</td>\n",
       "      <td>28</td>\n",
       "      <td>Rain showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>20:00</td>\n",
       "      <td>28</td>\n",
       "      <td>Light showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>21:00</td>\n",
       "      <td>27</td>\n",
       "      <td>Light showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>22:00</td>\n",
       "      <td>27</td>\n",
       "      <td>A few showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>23:00</td>\n",
       "      <td>27</td>\n",
       "      <td>Passing showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>00:00</td>\n",
       "      <td>27</td>\n",
       "      <td>A few showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>01:00</td>\n",
       "      <td>27</td>\n",
       "      <td>Light showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>02:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Thunderstorms. Overcast.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>03:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Rain showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>04:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Rain showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>05:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Rain showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>06:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Rain showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>07:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Rain showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>08:00</td>\n",
       "      <td>26</td>\n",
       "      <td>A few showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>09:00</td>\n",
       "      <td>28</td>\n",
       "      <td>A few showers. Overcast.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>10:00</td>\n",
       "      <td>29</td>\n",
       "      <td>Overcast.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>11:00</td>\n",
       "      <td>30</td>\n",
       "      <td>Overcast.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>12:00</td>\n",
       "      <td>30</td>\n",
       "      <td>Overcast.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>13:00</td>\n",
       "      <td>31</td>\n",
       "      <td>Isolated tstorms. Overcast.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>14:00</td>\n",
       "      <td>31</td>\n",
       "      <td>Isolated tstorms. Overcast.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   Time  Temperature            Weather Condition  \\\n",
       "0   2024-04-18  16:00           30    Thundershowers. Overcast.   \n",
       "1   2024-04-18  17:00           30    Thundershowers. Overcast.   \n",
       "2   2024-04-18  18:00           29    Thundershowers. Overcast.   \n",
       "3   2024-04-18  19:00           28      Rain showers. Overcast.   \n",
       "4   2024-04-18  20:00           28     Light showers. Overcast.   \n",
       "5   2024-04-18  21:00           27     Light showers. Overcast.   \n",
       "6   2024-04-18  22:00           27     A few showers. Overcast.   \n",
       "7   2024-04-18  23:00           27   Passing showers. Overcast.   \n",
       "8   2024-04-18  00:00           27     A few showers. Overcast.   \n",
       "9   2024-04-18  01:00           27     Light showers. Overcast.   \n",
       "10  2024-04-18  02:00           26     Thunderstorms. Overcast.   \n",
       "11  2024-04-18  03:00           26      Rain showers. Overcast.   \n",
       "12  2024-04-18  04:00           26      Rain showers. Overcast.   \n",
       "13  2024-04-18  05:00           26      Rain showers. Overcast.   \n",
       "14  2024-04-18  06:00           26      Rain showers. Overcast.   \n",
       "15  2024-04-18  07:00           26      Rain showers. Overcast.   \n",
       "16  2024-04-18  08:00           26     A few showers. Overcast.   \n",
       "17  2024-04-18  09:00           28     A few showers. Overcast.   \n",
       "18  2024-04-18  10:00           29                    Overcast.   \n",
       "19  2024-04-18  11:00           30                    Overcast.   \n",
       "20  2024-04-18  12:00           30                    Overcast.   \n",
       "21  2024-04-18  13:00           31  Isolated tstorms. Overcast.   \n",
       "22  2024-04-18  14:00           31  Isolated tstorms. Overcast.   \n",
       "\n",
       "    Is Heavy Rainfall  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  \n",
       "10                  0  \n",
       "11                  1  \n",
       "12                  1  \n",
       "13                  1  \n",
       "14                  1  \n",
       "15                  1  \n",
       "16                  1  \n",
       "17                  1  \n",
       "18                  0  \n",
       "19                  0  \n",
       "20                  0  \n",
       "21                  0  \n",
       "22                  0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta, date\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags and unwanted spaces from text.\"\"\"\n",
    "    text = re.sub('<[^<]+?>', '', text)  # Remove HTML tags\n",
    "    text = text.replace('&nbsp;', ' ')   # Replace non-breaking spaces\n",
    "    text = re.sub('\\s+', ' ', text)      # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "def extract_date(time_str, current_year):\n",
    "    \"\"\"Extract the date from the time string and append the current year.\"\"\"\n",
    "    match = re.search(r'\\d{2}:\\d{2}(\\w{3}, \\d{1,2} \\w{3})', time_str)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        date = datetime.strptime(date_str + f' {current_year}', '%a, %d %b %Y')\n",
    "        return date.strftime('%Y-%m-%d')\n",
    "    return None\n",
    "\n",
    "def read_and_process_data(file_path, starting_date=date.today().strftime('%Y-%m-%d')):\n",
    "    \"\"\"Read JSON data from a file and process it into a DataFrame.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        weather_data = json.load(file)\n",
    "    \n",
    "    cleaned_data = []\n",
    "    current_date = starting_date\n",
    "    last_time = None\n",
    "    for entry in weather_data:\n",
    "        time = clean_text(entry['time'])\n",
    "        if \"ConditionsComfortPrecipitation\" in time:\n",
    "            continue\n",
    "\n",
    "        new_date = extract_date(time, current_date[:4])\n",
    "        if new_date:\n",
    "            current_date = new_date\n",
    "            time = re.sub(r'\\w{3}, \\d{1,2} \\w{3}', '', time).strip()\n",
    "\n",
    "        if last_time and int(last_time[:2]) > int(time[:2]):\n",
    "            current_date = (datetime.strptime(current_date, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        temperature = clean_text(entry['temperature'])\n",
    "        weather_condition = entry['weatherCondition']\n",
    "        temperature = int(re.sub('[^\\d]', '', temperature))\n",
    "        \n",
    "        cleaned_data.append({\n",
    "            'Date': current_date,\n",
    "            'Time': time,\n",
    "            'Temperature': temperature,\n",
    "            'Weather Condition': weather_condition\n",
    "        })\n",
    "        last_time = time\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_data)\n",
    "    # Adding the 'Is Heavy Rainfall' column based on weather conditions\n",
    "    df['Is Heavy Rainfall'] = df['Weather Condition'].apply(lambda x: 1 if 'heavy' in x.lower() or 'shower' in x.lower() else 0)\n",
    "    return df\n",
    "\n",
    "def process_weather_data(file_path, starting_date=date.today().strftime('%Y-%m-%d')):\n",
    "    \"\"\"Orchestrate the process of reading, cleaning, and organizing weather data.\"\"\"\n",
    "    return read_and_process_data(file_path, starting_date)\n",
    "\n",
    "# Assuming your JSON file is named 'weather_data.json' in the current directory\n",
    "weather_df = process_weather_data('weather_data.json')\n",
    "weather_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare sg public holiday dataset in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holiday_df(year):\n",
    "    import datetime  # Importing datetime module within the function scope\n",
    "    start_date = datetime.date(year, 1, 1)\n",
    "    end_date = datetime.date(year, 12, 31)\n",
    "\n",
    "    # List of holidays in Singapore for the given year\n",
    "    holidays_sg = [\n",
    "        datetime.date(year, 1, 1), datetime.date(year, 2, 10), datetime.date(year, 2, 11),\n",
    "        datetime.date(year, 3, 29), datetime.date(year, 4, 10), datetime.date(year, 5, 1),\n",
    "        datetime.date(year, 5, 20), datetime.date(year, 6, 17), datetime.date(year, 8, 9),\n",
    "        datetime.date(year, 10, 31), datetime.date(year, 12, 25)\n",
    "    ]\n",
    "\n",
    "    # Dictionary to convert day names to numbers\n",
    "    day_to_number = {\n",
    "        'Monday': 1,\n",
    "        'Tuesday': 2,\n",
    "        'Wednesday': 3,\n",
    "        'Thursday': 4,\n",
    "        'Friday': 5,\n",
    "        'Saturday': 6,\n",
    "        'Sunday': 7\n",
    "    }\n",
    "\n",
    "    holiday_sg_24 = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        day_of_week = current_date.strftime('%A')\n",
    "        holiday_flg_sg = 1 if current_date in holidays_sg else 0\n",
    "        holiday_sg_24.append([current_date.strftime('%Y-%m-%d'), day_of_week, holiday_flg_sg])\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "\n",
    "    # Create DataFrame\n",
    "    holiday_sg_24 = pd.DataFrame(holiday_sg_24, columns=['calendar_date', 'day_of_week', 'holiday_flg_sg'])\n",
    "\n",
    "    # Map day_of_week from name to number\n",
    "    holiday_sg_24['day_of_week'] = holiday_sg_24['day_of_week'].map(day_to_number)\n",
    "\n",
    "    return holiday_sg_24\n",
    "\n",
    "holiday_sg_24 = generate_holiday_df(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 15:09:00.659039: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-04-18 15:09:00.659171: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-18 15:09:00.659191: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-18 15:09:00.659225: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-18 15:09:00.659257: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 15:09:01.385331: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 2s - 469ms/step - loss: 0.2130 - val_loss: 0.2179\n",
      "Epoch 2/100\n",
      "5/5 - 0s - 42ms/step - loss: 0.1594 - val_loss: 0.1543\n",
      "Epoch 3/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.1104 - val_loss: 0.0964\n",
      "Epoch 4/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0756 - val_loss: 0.0576\n",
      "Epoch 5/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0655 - val_loss: 0.0492\n",
      "Epoch 6/100\n",
      "5/5 - 0s - 37ms/step - loss: 0.0683 - val_loss: 0.0477\n",
      "Epoch 7/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0632 - val_loss: 0.0478\n",
      "Epoch 8/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0556 - val_loss: 0.0517\n",
      "Epoch 9/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0557 - val_loss: 0.0507\n",
      "Epoch 10/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0528 - val_loss: 0.0476\n",
      "Epoch 11/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0491 - val_loss: 0.0426\n",
      "Epoch 12/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0450 - val_loss: 0.0403\n",
      "Epoch 13/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0400 - val_loss: 0.0393\n",
      "Epoch 14/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0371 - val_loss: 0.0386\n",
      "Epoch 15/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0358 - val_loss: 0.0383\n",
      "Epoch 16/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0339 - val_loss: 0.0368\n",
      "Epoch 17/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0306 - val_loss: 0.0362\n",
      "Epoch 18/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0264 - val_loss: 0.0367\n",
      "Epoch 19/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0253 - val_loss: 0.0377\n",
      "Epoch 20/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0258 - val_loss: 0.0381\n",
      "Epoch 21/100\n",
      "5/5 - 0s - 37ms/step - loss: 0.0235 - val_loss: 0.0390\n",
      "Epoch 22/100\n",
      "5/5 - 0s - 37ms/step - loss: 0.0225 - val_loss: 0.0395\n",
      "Epoch 23/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0245 - val_loss: 0.0396\n",
      "Epoch 24/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0216 - val_loss: 0.0396\n",
      "Epoch 25/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0224 - val_loss: 0.0394\n",
      "Epoch 26/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0227 - val_loss: 0.0389\n",
      "Epoch 27/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0241 - val_loss: 0.0389\n",
      "Epoch 28/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0239 - val_loss: 0.0382\n",
      "Epoch 29/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0236 - val_loss: 0.0378\n",
      "Epoch 30/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0225 - val_loss: 0.0373\n",
      "Epoch 31/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0237 - val_loss: 0.0369\n",
      "Epoch 32/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0216 - val_loss: 0.0367\n",
      "Epoch 33/100\n",
      "5/5 - 0s - 40ms/step - loss: 0.0242 - val_loss: 0.0365\n",
      "Epoch 34/100\n",
      "5/5 - 0s - 32ms/step - loss: 0.0224 - val_loss: 0.0363\n",
      "Epoch 35/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0222 - val_loss: 0.0363\n",
      "Epoch 36/100\n",
      "5/5 - 0s - 38ms/step - loss: 0.0225 - val_loss: 0.0361\n",
      "Epoch 37/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0219 - val_loss: 0.0359\n",
      "Epoch 38/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0211 - val_loss: 0.0357\n",
      "Epoch 39/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0201 - val_loss: 0.0355\n",
      "Epoch 40/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0220 - val_loss: 0.0352\n",
      "Epoch 41/100\n",
      "5/5 - 0s - 41ms/step - loss: 0.0204 - val_loss: 0.0352\n",
      "Epoch 42/100\n",
      "5/5 - 0s - 65ms/step - loss: 0.0221 - val_loss: 0.0350\n",
      "Epoch 43/100\n",
      "5/5 - 0s - 41ms/step - loss: 0.0207 - val_loss: 0.0352\n",
      "Epoch 44/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0209 - val_loss: 0.0348\n",
      "Epoch 45/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0208 - val_loss: 0.0346\n",
      "Epoch 46/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0225 - val_loss: 0.0346\n",
      "Epoch 47/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0204 - val_loss: 0.0341\n",
      "Epoch 48/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0214 - val_loss: 0.0339\n",
      "Epoch 49/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0212 - val_loss: 0.0339\n",
      "Epoch 50/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0210 - val_loss: 0.0339\n",
      "Epoch 51/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0196 - val_loss: 0.0339\n",
      "Epoch 52/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0210 - val_loss: 0.0334\n",
      "Epoch 53/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0207 - val_loss: 0.0334\n",
      "Epoch 54/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0206 - val_loss: 0.0331\n",
      "Epoch 55/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0207 - val_loss: 0.0332\n",
      "Epoch 56/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0202 - val_loss: 0.0338\n",
      "Epoch 57/100\n",
      "5/5 - 0s - 38ms/step - loss: 0.0201 - val_loss: 0.0338\n",
      "Epoch 58/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0201 - val_loss: 0.0339\n",
      "Epoch 59/100\n",
      "5/5 - 0s - 37ms/step - loss: 0.0193 - val_loss: 0.0340\n",
      "Epoch 60/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0183 - val_loss: 0.0338\n",
      "Epoch 61/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0206 - val_loss: 0.0336\n",
      "Epoch 62/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0226 - val_loss: 0.0342\n",
      "Epoch 63/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0212 - val_loss: 0.0336\n",
      "Epoch 64/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0221 - val_loss: 0.0333\n",
      "Epoch 65/100\n",
      "5/5 - 0s - 36ms/step - loss: 0.0211 - val_loss: 0.0339\n",
      "Epoch 66/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0212 - val_loss: 0.0335\n",
      "Epoch 67/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0188 - val_loss: 0.0331\n",
      "Epoch 68/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0212 - val_loss: 0.0330\n",
      "Epoch 69/100\n",
      "5/5 - 0s - 32ms/step - loss: 0.0209 - val_loss: 0.0332\n",
      "Epoch 70/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0203 - val_loss: 0.0328\n",
      "Epoch 71/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0206 - val_loss: 0.0326\n",
      "Epoch 72/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0199 - val_loss: 0.0331\n",
      "Epoch 73/100\n",
      "5/5 - 0s - 32ms/step - loss: 0.0217 - val_loss: 0.0330\n",
      "Epoch 74/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0204 - val_loss: 0.0330\n",
      "Epoch 75/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0208 - val_loss: 0.0331\n",
      "Epoch 76/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0208 - val_loss: 0.0328\n",
      "Epoch 77/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0212 - val_loss: 0.0325\n",
      "Epoch 78/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0202 - val_loss: 0.0327\n",
      "Epoch 79/100\n",
      "5/5 - 0s - 34ms/step - loss: 0.0194 - val_loss: 0.0325\n",
      "Epoch 80/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0197 - val_loss: 0.0325\n",
      "Epoch 81/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0183 - val_loss: 0.0327\n",
      "Epoch 82/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0199 - val_loss: 0.0326\n",
      "Epoch 83/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0205 - val_loss: 0.0325\n",
      "Epoch 84/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0188 - val_loss: 0.0325\n",
      "Epoch 85/100\n",
      "5/5 - 0s - 35ms/step - loss: 0.0193 - val_loss: 0.0326\n",
      "Epoch 86/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0203 - val_loss: 0.0324\n",
      "Epoch 87/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0205 - val_loss: 0.0325\n",
      "Epoch 88/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0188 - val_loss: 0.0324\n",
      "Epoch 89/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0209 - val_loss: 0.0324\n",
      "Epoch 90/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0191 - val_loss: 0.0322\n",
      "Epoch 91/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0199 - val_loss: 0.0323\n",
      "Epoch 92/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0202 - val_loss: 0.0332\n",
      "Epoch 93/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0204 - val_loss: 0.0325\n",
      "Epoch 94/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0204 - val_loss: 0.0323\n",
      "Epoch 95/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0205 - val_loss: 0.0322\n",
      "Epoch 96/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0206 - val_loss: 0.0328\n",
      "Epoch 97/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0190 - val_loss: 0.0323\n",
      "Epoch 98/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0209 - val_loss: 0.0321\n",
      "Epoch 99/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0206 - val_loss: 0.0323\n",
      "Epoch 100/100\n",
      "5/5 - 0s - 33ms/step - loss: 0.0203 - val_loss: 0.0320\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "Test RMSE: 67.953\n",
      "Reloading Tuner from my_dir/intro_to_kt/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras_tuner/src/tuners/hyperband.py:435: UserWarning: Model 'sequential' had a build config, but the model cannot be built automatically in `build_from_config(config)`. You should implement `def build_from_config(self, config)`, and you might also want to implement the method  that generates the config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the model (i.e. its variables) upon deserialization.\n",
      "  model.build_from_config(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0249 \n"
     ]
    }
   ],
   "source": [
    "# Load visitor data\n",
    "visitor_data = pd.read_csv(\"../data/raw/synthetic_visit_data.csv\")\n",
    "visitor_data['visit_date'] = pd.to_datetime(visitor_data['visit_date'])\n",
    "#visitor_data.head()\n",
    "\n",
    "weather_data = pd.read_csv(\"../data/processed/weather_data_cleaned.csv\")\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "columns_to_drop = weather_data.columns[0:3].tolist()  # Dropping columns by indices\n",
    "weather_data.drop(columns=columns_to_drop, inplace=True)\n",
    "#weather_data.head()\n",
    "\n",
    "holiday_data = pd.read_csv(\"../data/raw/date_info_2324.csv\")\n",
    "holiday_data['calendar_date'] = pd.to_datetime(holiday_data['calendar_date'])\n",
    "holiday_data.rename(columns={'calendar_date': 'calender_date'}, inplace=True)\n",
    "holiday_data.drop(columns=\"day_of_week\", inplace=True)\n",
    "#holiday_data.head()\n",
    "\n",
    "merged_data = pd.merge(visitor_data, weather_data, left_on='visit_date', right_on='Date', how='left')\n",
    "merged_data = pd.merge(merged_data, holiday_data, left_on='visit_date', right_on='calender_date', how='left')\n",
    "merged_data.drop(columns=[\"Date\",\"calender_date\",\"Highest 30 min Rainfall (mm)\",\"Highest 60 min Rainfall (mm)\",\"Highest 120 min Rainfall (mm)\",\"Maximum Temperature (°C)\",\"Minimum Temperature (°C)\",\"Mean Wind Speed (km/h)\",\"Max Wind Speed (km/h)\"], inplace=True)\n",
    "merged_data.head()\n",
    "\n",
    "# 1. Encoding Categorical Variables\n",
    "df = pd.get_dummies(merged_data, columns=['day_of_week'])\n",
    "\n",
    "# 2. Creating a binary feature for heavy rainfall\n",
    "merged_data['heavy_rainfall_flg'] = (merged_data['Daily Rainfall Total (mm)'] > 20).astype(int)\n",
    "\n",
    "# 3. Categorizing temperature\n",
    "merged_data['temperature_category'] = pd.cut(df['Mean Temperature (°C)'],\n",
    "                                    bins=[-np.inf, 15, 25, np.inf],\n",
    "                                    labels=['cold', 'mild', 'hot'])\n",
    "\n",
    "# 4. Consolidating Holiday Flags\n",
    "merged_data['is_holiday'] = merged_data[['holiday_flg_sg', 'holiday_flg_cn', 'holiday_flg_in']].max(axis=1)\n",
    "\n",
    "# 5. Adding week of the year\n",
    "merged_data['week_of_year'] = pd.to_datetime(merged_data['visit_date']).dt.isocalendar().week\n",
    "\n",
    "# It's essential to transform these new categorical variables into a form suitable for modeling\n",
    "merged_data = pd.get_dummies(merged_data, columns=['temperature_category'])\n",
    "\n",
    "merged_data.drop(columns=[\"Daily Rainfall Total (mm)\",\"Mean Temperature (°C)\", \"holiday_flg_sg\", \"holiday_flg_cn\", \"holiday_flg_in\"], inplace = True)\n",
    "merged_data.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_percent = 0.8  # Use 80% of the data for training\n",
    "split_index = int(len(merged_data) * train_percent)\n",
    "train_data = visitor_data.iloc[:split_index]\n",
    "test_data = visitor_data.iloc[split_index:]\n",
    "\n",
    "# Convert the index (visit_date) to datetime\n",
    "merged_data.index = pd.to_datetime(merged_data.index)\n",
    "\n",
    "# Create lag features for visitors\n",
    "for lag in range(1, 8):  # 7 days lag\n",
    "    merged_data[f'visitors_lag_{lag}'] = merged_data['visitors'].shift(lag)\n",
    "\n",
    "# Create rolling window features (7-day rolling mean and std deviation)\n",
    "merged_data['rolling_mean_visitors'] = merged_data['visitors'].rolling(window=7).mean().shift(1)\n",
    "merged_data['rolling_std_visitors'] = merged_data['visitors'].rolling(window=7).std().shift(1)\n",
    "\n",
    "# Fill any NaN values that have been introduced by lag/rolling features\n",
    "merged_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "merged_data['year'] = merged_data['visit_date'].dt.year\n",
    "merged_data['month'] = merged_data['visit_date'].dt.month\n",
    "merged_data['day'] = merged_data['visit_date'].dt.day\n",
    "lstm_data = merged_data.drop(['visit_date','day_of_week'], axis=1, inplace=False)\n",
    "X = lstm_data.drop(['visitors'], axis=1)\n",
    "y = lstm_data['visitors']\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.values.reshape(-1,1))\n",
    "X_scaled = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select features and target\n",
    "X = lstm_data.drop(['visitors'], axis=1)\n",
    "y = lstm_data['visitors']\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.values.reshape(-1,1))\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_scaled = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model with dropout\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.1, verbose=2)\n",
    "\n",
    "''''\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "# Predict and inverse transform predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions_inv = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Inversely transform y_test for comparison\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, predictions_inv))\n",
    "print(f'Test RMSE: {rmse:.3f}')\n",
    "\n",
    "'''\n",
    "# Plot predictions against actual values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(predictions_inv, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units1', min_value=50, max_value=200, step=50),\n",
    "                   return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(rate=hp.Float('dropout1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Int('units2', min_value=20, max_value=100, step=20), return_sequences=False))\n",
    "    model.add(Dropout(rate=hp.Float('dropout2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "loss = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  1.,  0.,  0.,  1.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediction_data(visitor_data, weather_df, holiday_sg_24):\n",
    "    visitor_data['visit_date'] = pd.to_datetime(visitor_data['visit_date'])\n",
    "    weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "    holiday_sg_24['calendar_date'] = pd.to_datetime(holiday_sg_24['calendar_date'])\n",
    "    prediction_df = pd.merge(visitor_data, weather_df, left_on='visit_date', right_on='Date', how='right')\n",
    "    prediction_df = pd.merge(prediction_df, holiday_sg_24, left_on='Date', right_on='calendar_date', how='left')\n",
    "    prediction_df['day_of_week'] = prediction_df['Date'].dt.day_name()\n",
    "    prediction_df['week_of_year'] = pd.to_datetime(prediction_df['Date']).dt.isocalendar().week\n",
    "    prediction_df.drop_duplicates(inplace=True)\n",
    "    prediction_df['is_heavy_rainfall_flg'] = prediction_df['Weather Condition'].apply(lambda x: 1 if 'shower' in x else 0)\n",
    "    prediction_df.drop(columns=['visit_date', 'Weather Condition','Time','day_of_week_y', 'calendar_date','day_of_week_x'], inplace=True)\n",
    "    prediction_df.rename(columns={'Date': 'visit_date', 'holiday_flg_sg': 'is_holiday'}, inplace=True)\n",
    "    prediction_df['temperature_category'] = pd.cut(prediction_df['Temperature'],\n",
    "                                        bins=[-np.inf, 15, 25, np.inf],\n",
    "                                        labels=['cold', 'mild', 'hot'])\n",
    "    prediction_df.drop(columns='Temperature',inplace=True)\n",
    "    prediction_df = pd.get_dummies(prediction_df, columns=['temperature_category'])\n",
    "    \n",
    "    # Create lag features based on your model needs\n",
    "    for lag in range(1, 8):  # Example: Creating 7 days of lag features\n",
    "        prediction_df[f'visitors_lag_{lag}'] = prediction_df['visitors'].shift(lag)\n",
    "\n",
    "    # Rolling window features (mean and std deviation)\n",
    "    prediction_df['rolling_mean_visitors'] = prediction_df['visitors'].rolling(window=7).mean().shift(1)\n",
    "    prediction_df['rolling_std_visitors'] = prediction_df['visitors'].rolling(window=7).std().shift(1)\n",
    "\n",
    "    # Fill any NaN values that have been introduced by lag/rolling features\n",
    "    prediction_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Adding date parts\n",
    "    prediction_df['year'] = prediction_df['visit_date'].dt.year\n",
    "    prediction_df['month'] = prediction_df['visit_date'].dt.month\n",
    "    prediction_df['day'] = prediction_df['visit_date'].dt.day\n",
    "\n",
    "    # Prepare feature matrix X for the LSTM model\n",
    "    X = prediction_df.drop(['visit_date', 'day_of_week'], axis=1)\n",
    "    X_scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(X)\n",
    "    X_scaled = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "weather_df = process_weather_data('weather_data.json')\n",
    "holiday_sg_24 = generate_holiday_df(2024)\n",
    "visitor_data = pd.read_csv(\"../data/raw/synthetic_visit_data.csv\")\n",
    "X_predict = prediction_data(visitor_data, weather_df, holiday_sg_24)\n",
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(X_predict)\n",
    "#predictions = best_model.predict(X_predict) \n",
    "'用X_predict的时候kernal crash了。目前用X_test先跑'\n",
    "\n",
    "predictions_inv = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Create a DataFrame for predictions 7 days from today\n",
    "future_dates = pd.date_range(start=datetime.today() + timedelta(days=1), periods=7, freq='D')\n",
    "future_predictions = pd.DataFrame({\n",
    "    'date': future_dates,\n",
    "    'predicted_visitors': np.round(predictions_inv.flatten()[:7])  # Assuming you have at least 7 predictions\n",
    "})\n",
    "\n",
    "# Function to simulate hourly arrival based on daily visitor prediction\n",
    "def simulate_hourly_arrival(predicted_daily_visitors):\n",
    "    time_intervals_1 = np.linspace(8, 16, 9)  # From 8 AM to 4 PM (9 hours)\n",
    "    time_intervals_2 = np.linspace(17, 23, 7)  # From 5 PM to 11 PM (7 hours)\n",
    "    time_intervals_strings = [f'{int(hour)}:00' for hour in np.concatenate((time_intervals_1, time_intervals_2), axis=None)]\n",
    "    \n",
    "    visitor_counts_1 = skewnorm.pdf(time_intervals_1, a=2, loc=12)  # Adjust skew and location as needed\n",
    "    visitor_counts_2 = skewnorm.pdf(time_intervals_2, a=5, loc=19)\n",
    "    visitor_counts_1 = visitor_counts_1 / visitor_counts_1.max()  # Normalize to [0, 1]\n",
    "    visitor_counts_2 = visitor_counts_2 / visitor_counts_2.max()\n",
    "    \n",
    "    visitor_counts_1 = np.round(visitor_counts_1 * predicted_daily_visitors * 0.6 / visitor_counts_1.sum()).astype(int)  # Adjust ratio as needed\n",
    "    visitor_counts_2 = np.round(visitor_counts_2 * predicted_daily_visitors * 0.4 / visitor_counts_2.sum()).astype(int)\n",
    "    \n",
    "    hourly_counts = np.concatenate((visitor_counts_1, visitor_counts_2), axis=None)\n",
    "    return time_intervals_strings, hourly_counts\n",
    "\n",
    "def generate_and_merge_hourly_data(future_predictions, holiday_sg_24):\n",
    "    # Empty DataFrame to store the results\n",
    "    output_df = pd.DataFrame(columns=['date', 'time', 'estimated_arrival_count'])\n",
    "\n",
    "    # Generate hourly data for each predicted day\n",
    "    for idx, row in future_predictions.iterrows():\n",
    "        time_strings, hourly_visitors = simulate_hourly_arrival(row['predicted_visitors'])\n",
    "        day_df = pd.DataFrame({\n",
    "            'date': [row['date']] * len(time_strings),\n",
    "            'time': time_strings,\n",
    "            'estimated_arrival_count': hourly_visitors\n",
    "        })\n",
    "        output_df = pd.concat([output_df, day_df], ignore_index=True)\n",
    "\n",
    "    # Ensure the 'date' columns are in datetime format\n",
    "    output_df['date'] = pd.to_datetime(output_df['date']).dt.date\n",
    "    holiday_sg_24['calendar_date'] = pd.to_datetime(holiday_sg_24['calendar_date']).dt.date\n",
    "\n",
    "    # Merge output_df with holiday_sg_24 on date\n",
    "    merged_df = pd.merge(output_df, holiday_sg_24, left_on='date', right_on='calendar_date', how='left')\n",
    "\n",
    "    # Drop the extra date columns if no longer needed\n",
    "    merged_df.drop(columns=['calendar_date'], inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "result_df = generate_and_merge_hourly_data(future_predictions, holiday_sg_24)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
